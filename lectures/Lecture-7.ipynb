{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7. Solvers for large-scale sparse systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture\n",
    "- PDEs discretization\n",
    "- Sparse matrices\n",
    "- Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "- Direct solvers\n",
    "- Iterative solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical problems we need to solve\n",
    "\n",
    "1. Solving linear systems $Ax = f$\n",
    "2. Solving eigenvalue problem $A x_i = \\lambda x_i$\n",
    "3. Computing matrix functions, i.e. $y = e^{At} y_0$\n",
    "\n",
    "To solve 2 and 3 we need to solve auxilliary linear systems. \n",
    "\n",
    "Types of solvers\n",
    "- iterative: matrix-by-vector product is **easy**, but the matrix is typically **ill-conditioned**, thus iterative methods will converge slowly, and **preconditioners** are needed\n",
    "- direct: no problem with ill-conditioning, but complexity is not optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast direct solvers\n",
    "\n",
    "One of the research directions for solving sparse linear systems\n",
    "\n",
    "$$\n",
    "A x = f\n",
    "$$\n",
    "\n",
    "with a given **sparse matrix $A$** are so-called **direct solvers** which try to factorize the matrix $A$.\n",
    "\n",
    "The simplest decomposition is the **sparse LU** factorization of the form\n",
    "\n",
    "$$A = LU,$$\n",
    "\n",
    "where $L$ is **sparse lower triangular**, and $U$ is **sparse upper triangular**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Gaussian elimination and graphs\n",
    "\n",
    "The crucial concept to analyze the efficiency of sparse matrix factorization is the **sparse matrix graph**.\n",
    "\n",
    "The graph of the sparse matrix has vertices $i$, and the edge exists if $A_{ij} \\ne 0$.\n",
    "\n",
    "The pattern of the $L$ factor can be inferred from the **symbolic** operations in the sparse matrix graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph of the sparse matrix\n",
    "\n",
    "For simplicity, consider **Cholesky factorization** for a **symmetric positive-definite matrix** $A$:\n",
    "\n",
    "$$A = LL^{\\top}.$$\n",
    "\n",
    "The positions of non-zero elements can be inferred from the code\n",
    "\n",
    "```\n",
    "for j from 1 to N:\n",
    "   Add edges between j's higher-order neighbors\n",
    "```\n",
    "\n",
    "<img src='pic/screen.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Permutation selection\n",
    "\n",
    "- The fill-in of a matrix are those entries which change from an initial zero to a nonzero value during the execution of an algorithm.\n",
    "\n",
    "- The fill-in is different for different permutations. So, before factorization we need to find reordering which produces the smallest fill-in.\n",
    "\n",
    "- For one order you get sparse factorization, for another - dense factors.\n",
    "\n",
    "In fact, for a Cholesky factorization you compute\n",
    "\n",
    "$$A = P L L^{\\top} P^{\\top},$$\n",
    "\n",
    "where $P$ is a **permutation matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2D model problem\n",
    "\n",
    "- 2D model problem: Poisson equation on $n \\times n$ finite difference grid\n",
    "- Total number of unknowns $n^2 = N$\n",
    "- Theoretical results from the fill-in:\n",
    "  - Natural (no) permutation: $\\mathcal{O}(N^{3/2})$\n",
    "  - Random permutation: expected value is $\\mathcal{O}(N \\log N)$\n",
    "  - Nested dissection permutation: $\\mathcal{O}(N)$.\n",
    "  \n",
    "  By **fill-in** I mean the number of non-zeros in the $L$ factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "from scipy.sparse import csc_matrix, csr_matrix, coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n = 20\n",
    "ex = np.ones(n);\n",
    "lp1 = sp.sparse.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
    "e = sp.sparse.eye(n)\n",
    "A = sp.sparse.kron(lp1, e) + sp.sparse.kron(e, lp1)\n",
    "A = csc_matrix(A)\n",
    "T = scipy.sparse.linalg.spilu(A)\n",
    "plt.spy(T.L, marker='.', color='k', markersize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested dissection ordering\n",
    "\n",
    " A separator in a graph $G$ is a set $S$ of vertices whose removal leaves at\n",
    "least two connected components\n",
    "- A nested dissection ordering for an $N$-vertex graph $G$ numbers its\n",
    "vertices from $1$ to $N$ as follows:\n",
    "- Find a separator $S$, whose removal leaves connected components\n",
    "$T_1$, $T_2$, $\\ldots$, $T_k$\n",
    "- Number the vertices of $S$ from $N âˆ’ |S| + 1$ to $N$\n",
    "- Recursively, number the vertices of each component: $T_1$ from $1$ to\n",
    "$|T_1|$, $T_2$ from $|T_1| + 1$ to $|T_1| + |T_2|$, etc\n",
    "- If a component is small enough, number it arbitrarily\n",
    "\n",
    "It all boils down to finding good separators! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested dissection is \"approximately optimal\"\n",
    "\n",
    "From theory, nested dissection gives you optimal complexity. \n",
    "\n",
    "Again, there are other methods that win for medium-sized problems.\n",
    "\n",
    "They are based on **heuristic** matrix reordering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Banded reordering\n",
    "\n",
    "Make the matrix more \"banded\" (Reverse Cuhill-McKee, Sloan, etc.). \n",
    "\n",
    "The idea is to try to keep entries closer to the diagonal. \n",
    "\n",
    "Works well for matrices coming from \"quasi-one dimensional\" PDEs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimal degree orderings\n",
    "\n",
    "The idea is to eliminate rows and/or columns with fewer non-zeros, update fill-in and then repeat\n",
    "\n",
    "Efficient implementation is an issue (adding/removing elements).\n",
    "\n",
    "Current champion is \"approximate minimal degree\" by [Amestoy, Davis, Duff](https://pdfs.semanticscholar.org/606a/5dce82d9953aa18b732691b427231b87be90.pdf)\n",
    "\n",
    "It is **suboptimal** even for 2D problems\n",
    "\n",
    "In practice, often wins for medium-sized problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nested dissection\n",
    "\n",
    "- Find a separator, number it last, proceed recursively.\n",
    "- In theory, optimal.\n",
    "- In practice, beats others for very large problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solving sparse matrices, coming from PDEs\n",
    "\n",
    "<img src='pic/complexity2.png' /img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separators in practice\n",
    "\n",
    "Computing separators is not a **trivial task**.\n",
    "\n",
    "Graph partitioning heuristics have been an active research area for many years, often motivated by partitioning for parallel computation.\n",
    "\n",
    "Existing approaches:\n",
    "\n",
    "- Spectral partitioning (uses eigenvectors of Laplacian matrix of graph)\n",
    "- Geometric partitioning (for meshes with specified vertex coordinates)\n",
    "- Iterative-swapping (Kernighan-Lin, Fiduccia-Matheysses)\n",
    "- Breadth-first search \n",
    "\n",
    "Many popular modern codes (e.g. Metis, Chaco) use multilevel iterative swapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iterative swapping\n",
    "\n",
    "The \"cost\" of the separator is defined in a very natural way as the sum over edges:\n",
    "\n",
    "$$T(A, B) = \\sum_{e} \\{ \\mbox{weight}(e): \\mbox{ $e$ connects $A$ and $B$} \\}.$$\n",
    "\n",
    "Given some initial partion, test some subsets $X$ and $Y$ of the same size, and if swapping decreases the cost function - swap them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spectral bisection\n",
    "\n",
    "The idea of spectral bisection goes back to Fiedler.\n",
    "\n",
    "We introduce the **graph Laplacian** of the matrix, which is defined as as symmetric matrix\n",
    "\n",
    "that\n",
    "\n",
    "$$L_{ii} = \\mbox{degree of node $i$},$$\n",
    "\n",
    "$$L_{ij} = -1, \\mbox{if $i \\ne j$  and there is an edge},$$\n",
    "\n",
    "and $0$ otherwise.\n",
    "\n",
    "- Rows of $L$ sum to zero, thus there is an eigenvalue $1$, \n",
    "- Eigenvalues are non-negative.\n",
    "- The number of connected components of a graph is the number of **zero eigenvalues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When to use sparse direct solvers\n",
    "\n",
    "- When to use sparse direct solvers?\n",
    "\n",
    "- They typically work well for 1D/2D problems and \"not so large\" 3D problems. The problem with memory becomes very harsh for 3D problems, thus other methods are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structure of sparse matrices, coming from PDEs\n",
    "\n",
    "For a general sparse matrix, fast direct method is typically the method of choice.\n",
    "\n",
    "The a sparse matrix coming from a PDE, the rows/columns can be associated with **points** (elements) in a $d$-dimensional space, just like for the H-matrix case.\n",
    "\n",
    "This is an additional structure that can be used in many ways, for example by approximating factors using $H$-matrix case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast direct solvers and $\\mathcal{H}$-matrices\n",
    "\n",
    "Inverse of a sparse matrix is typically not sparse, but e.g. for PDEs it has lowrank blocks ($\\mathcal{H}, \\mathcal{H}^2$-matrix). This information can be utilized to build efficient solver. \n",
    "\n",
    "Details on the blackboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.  1.  0.  0.  0.  0.  0.]\n",
      " [ 1. -2.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1. -2.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1. -2.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1. -2.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1. -2.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1. -2.]]\n",
      "[[-0.875 -0.75  -0.625 -0.5   -0.375 -0.25  -0.125]\n",
      " [-0.75  -1.5   -1.25  -1.    -0.75  -0.5   -0.25 ]\n",
      " [-0.625 -1.25  -1.875 -1.5   -1.125 -0.75  -0.375]\n",
      " [-0.5   -1.    -1.5   -2.    -1.5   -1.    -0.5  ]\n",
      " [-0.375 -0.75  -1.125 -1.5   -1.875 -1.25  -0.625]\n",
      " [-0.25  -0.5   -0.75  -1.    -1.25  -1.5   -0.75 ]\n",
      " [-0.125 -0.25  -0.375 -0.5   -0.625 -0.75  -0.875]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as spsp\n",
    "n = 7\n",
    "ex = np.ones(n);\n",
    "a = spsp.spdiags(np.vstack((ex,  -2*ex, ex)), [-1, 0, 1], n, n, 'csr'); \n",
    "a = a.todense()\n",
    "b = np.array(np.linalg.inv(a))\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iterative methods\n",
    "\n",
    "If we want to achieve $\\mathcal{O}(N)$ complexity, then direct solvers are not appropriate.\n",
    "\n",
    "If we want to solve partial eigenproblem, the full eigendecomposition is too costly.\n",
    "\n",
    "For both problems we will use iterative, Krylov subspace solvers, which treat the matrix as a **black-box** linear operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix as a black box\n",
    "\n",
    "We have now an absolutely different view on a matrix: matrix is now a **linear operator**, that acts on a vector,  \n",
    "and this action can be computed in $\\mathcal{O}(N)$ operations.\n",
    "\n",
    "**This is the only information** we know about the matrix: the <font color='red'> matrix-by-vector product (matvec) </font>\n",
    "\n",
    "Can we solve linear systems using only matvecs?\n",
    "\n",
    "Of course, we can multiply by the colums of the identity matrix, and recover the full matrix, but it is not what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Krylov subspace methods\n",
    "\n",
    "-  Krylov subspace of the matrix $A$ given vector $f$ is defined as \n",
    "$$\n",
    "   K_m(A, f) = \\mathrm{Span}(f, Af, A^2 f, \\ldots, A^{m-1}f )\n",
    "$$\n",
    "- Krylov methods (known from NLA course):\n",
    "    - CG works for SPD matrices, minimizes energy functional over Krylov subspace\n",
    "    - Minres works for symmetric matrices, minimizes residual\n",
    "    - GMRES works for general matrices, also minimizes residual but does not have recurrent formulas $\\to$ restarting  \n",
    "    - BiCGstab works for general matrices, does not minimize functional but has short recurrent formulas\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Nonlinear GMRES\" or Anderson acceleration\n",
    "\n",
    "We can apply the GMRES-like idea to speed up the convergence of a given fixed-point iteration\n",
    "\n",
    "$$x_{k+1} = \\Phi(x_k).$$\n",
    "\n",
    "This was actually older than the GMRES, and known as an Direct Inversion in Iterated Subspaces in Quantum Chemistry, or **Anderson Acceleration**.\n",
    "\n",
    "Idea: \n",
    "\n",
    "**Use history** for the update, \n",
    "\n",
    "$$x_{k+1} = \\Phi(x_k) + \\sum_{s=1}^m \\alpha_s (x_{k - s} - \\Phi(x_{k - s})), $$\n",
    "\n",
    "and the parameters $\\alpha_s$ are selected to minimize the norm of the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Battling the condition number\n",
    "\n",
    "The condition number problem is **un-avoidable** if only the matrix-by-vector product is used.\n",
    "\n",
    "Thus we need an **army of preconditioners** to solve it.\n",
    "\n",
    "There are several **general purpose** preconditioners that we can use,\n",
    "\n",
    "but often for a particular problem a special design is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preconditioner: general concept\n",
    "\n",
    "The general concept of the preconditioner is simple:\n",
    "\n",
    "Given a linear system \n",
    "\n",
    "$$A x = f,$$\n",
    "\n",
    "we want to find the matrix $P_R$ (or $P_L$) such that \n",
    "\n",
    "1. Condition number of $AP_R^{-1}$ (right preconditioner) or $P^{-1}_LA$ (right preconditioner) or $P^{-1}_L A P_R^{-1}$ is better than for $A$\n",
    "2. We can easily solve $P_Ly = g$ or $P_Ry = g$ for any $g$ (otherwise we could choose e.g. $P_L = A$)\n",
    "\n",
    "Then we solve for (right preconditioner)\n",
    "\n",
    "$$ AP_R^{-1} y = f \\quad \\Rightarrow \\quad P_R x = y$$ \n",
    "\n",
    "or  (left preconditioner)\n",
    "\n",
    "$$ P_L^{-1} A x = P_L^{-1}f,$$ \n",
    "or even both\n",
    "$$ P_L^{-1} A P_R^{-1} y = P_L^{-1}f \\quad \\Rightarrow \\quad P_R x = y.$$ \n",
    "\n",
    "The best choice is of course $P = A,$ but this does not make life easier.\n",
    "\n",
    "One of the ideas is to use other iterative methods (beside Krylov) as preconditioners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incomplete LU\n",
    "\n",
    "\n",
    "Suppose you want to eliminate a variable $x_1$,\n",
    "\n",
    "and the equations have the form\n",
    "\n",
    "$$5 x_1 + x_4 + x_{10} = 1, \\quad 3 x_1 + x_4 + x_8 = 0, \\ldots,$$\n",
    "\n",
    "and in all other equations $x_1$ are not present. \n",
    "\n",
    "After the elimination, only $x_{10}$ will enter additionally to the second equation (new fill-in).\n",
    "\n",
    "$$x_4 + x_8 + 3(1 - x_4 - x_{10})/5 = 0$$\n",
    "\n",
    "In the Incomplete $LU$ case (actually, ILU(0)) we just throw away the **new fill-in**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incomplete-LU: formal definition\n",
    "\n",
    "We run the usual LU-decomposition cycle, but avoid inserting non-zeros **other** than the initial non-zero pattern. \n",
    "\n",
    "```python\n",
    "    L = np.zeros((n, n))\n",
    "    U = np.zeros((n, n))\n",
    "    for k in range(n): #Eliminate one row   \n",
    "        L[k, k] = 1\n",
    "        for i in range(k+1, n):\n",
    "            L[i, k] = a[i, k] / a[k, k]\n",
    "            for j in range(k+1, n):\n",
    "                a[i, j] = a[i, j] - L[i, k] * a[k, j]  #New fill-ins appear here\n",
    "        for j in range(k, n):\n",
    "            U[k, j] = a[k, j]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ILU(k)\n",
    "\n",
    "Yousef Saad (who is the author of GMRES) also had a seminal paper on the **Incomplete LU** decomposition\n",
    "\n",
    "A good book on the topic is Saad, Yousef (1996), Iterative methods for sparse linear systems \n",
    "\n",
    "And he proposed **ILU(k)** method, which has a nice interpretation in terms of graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ILU(k): idea\n",
    "\n",
    "The idea of ILU(k) is very instructive and is based on the connection between sparse matrices and graphs.\n",
    "\n",
    "Suppose you have an $n \\times n$ matrix $A$ and a corresponding adjacency graph.\n",
    "\n",
    "Then we eliminate one variable (vertex) and get a smaller system of size $(n-1) \\times (n-1)$.\n",
    "\n",
    "New edges (=fill-in) appears between high-order neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LU & graphs\n",
    "\n",
    "The new edge can appear only between vertices that had common neighbour: it means, that they are second-order neigbours.  This is also a sparsity pattern of the matrix \n",
    "\n",
    "$$A^2.$$\n",
    "\n",
    "The **ILU(k)** idea is to leave only the elements in $L$ and $U$ that are $k$-order neighbours in the original graph.\n",
    "\n",
    "The ILU(2) is very efficient, but for some reason completely abandoned (i.e. there is no implementation in MATLAB and scipy).\n",
    "\n",
    "There is an original Sparsekit software by Saad, which works quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ILU Thresholded (ILUT)\n",
    "A much more popular approach is based on the so-called **thresholded LU**.\n",
    "\n",
    "You do the standard Gaussian elimination with fill-ins, but either:\n",
    "\n",
    "- Throw away elements that are smaller than threshold, and/or control the amount of non-zeros you are allowed to store.\n",
    "\n",
    "- The smaller is the threshold, the better is the preconditioner, but more memory it takes.\n",
    "\n",
    "It is denoted ILUT($\\tau$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symmetric positive definite case\n",
    "\n",
    "In the SPD case, instead of incomplete LU you should use Incomplete Cholesky, which is twice faster and consumes twice less memory.\n",
    "\n",
    "Both **ILUT** and **Ichol** are implemented in scipy and you can try (nothing quite fancy, but it works)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second-order LU preconditioners\n",
    "\n",
    "There is a more efficient (but much less popular due to the limit of open-source implementations) **second-order** LU factorization [proposed by I. Kaporin](http://www.researchgate.net/profile/I_Kaporin/publication/242940993_High_quality_preconditioning_of_a_general_symmetric_positive_definite_matrix_based_on_its_UTU__UTR__RTU-decomposition/links/53f72ad90cf2888a74976f54.pdf)\n",
    "\n",
    "The idea is to approximate the matrix in the form\n",
    "\n",
    "$$A \\approx U_2 U^{\\top}_2 + U^{\\top}_2 R_2 + R^{\\top}_2 U_2,$$\n",
    "\n",
    "which is just the expansion of the $UU^{\\top}$ with respect to the perturbation of $U$. \n",
    "\n",
    "where $U_1$ and $U_2$ are low-triangular and sparse, whereare $R_2$ is small with respect to the drop tolerance parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Summary\n",
    "- Fast direct solvers are often the method of choice for small-sized problems.\n",
    "- Iterative methods\n",
    "- Preconditioning"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
